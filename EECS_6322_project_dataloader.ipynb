{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPF5DZGMJG3ifQb4JKLbUtK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElmiraOn/EECS6322-Global-Rhythm-Style-TransferWithout-Text-Transcriptions/blob/main/EECS_6322_project_dataloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "Y12tsLgRDXyd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY8gdL_3boN0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from numpy.random import uniform\n",
        "from torch.utils import data\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from multiprocessing import Process, Manager"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Utterances(data.Dataset):\n",
        "    \"\"\"Dataset class for the Utterances dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, hparams):\n",
        "        \"\"\"Initialize and preprocess the Utterances dataset.\"\"\"\n",
        "        self.meta_file = hparams.meta_file\n",
        "\n",
        "        self.feat_dir_1 = hparams.feat_dir_1\n",
        "        self.feat_dir_2 = hparams.feat_dir_2\n",
        "        self.feat_dir_3 = hparams.feat_dir_3\n",
        "\n",
        "        self.step = 4\n",
        "        self.split = 0\n",
        "\n",
        "        self.max_len_pad = hparams.max_len_pad\n",
        "\n",
        "        meta = pickle.load(open(self.meta_file, \"rb\"))\n",
        "\n",
        "        manager = Manager()\n",
        "        meta = manager.list(meta)\n",
        "        dataset = manager.list(len(meta)*[None])  # <-- can be shared between processes.\n",
        "        processes = []\n",
        "        for i in range(0, len(meta), self.step):\n",
        "            p = Process(target=self.load_data,\n",
        "                        args=(meta[i:i+self.step],dataset,i))\n",
        "            p.start()\n",
        "            processes.append(p)\n",
        "        for p in processes:\n",
        "            p.join()\n",
        "\n",
        "        # very importtant to do dataset = list(dataset)\n",
        "        self.train_dataset = list(dataset)\n",
        "        self.num_tokens = len(self.train_dataset)\n",
        "\n",
        "        print('Finished loading the {} Utterances training dataset...'.format(self.num_tokens))\n",
        "\n",
        "\n",
        "    def load_data(self, submeta, dataset, idx_offset):\n",
        "        for k, sbmt in enumerate(submeta):\n",
        "            uttrs = len(sbmt)*[None]\n",
        "            for j, tmp in enumerate(sbmt):\n",
        "                if j < 2:\n",
        "                    # fill in speaker name and embedding\n",
        "                    uttrs[j] = tmp\n",
        "                else:\n",
        "                    # fill in data\n",
        "                    sp_tmp = np.load(os.path.join(self.feat_dir_1, tmp))\n",
        "                    cep_tmp = np.load(os.path.join(self.feat_dir_2, tmp))[:, 0:14]\n",
        "                    cd_tmp = np.load(os.path.join(self.feat_dir_3, tmp))\n",
        "\n",
        "                    assert len(sp_tmp) == len(cep_tmp) == len(cd_tmp)\n",
        "\n",
        "                    uttrs[j] = ( np.clip(sp_tmp, 0, 1), cep_tmp, cd_tmp )\n",
        "            dataset[idx_offset+k] = uttrs\n",
        "\n",
        "\n",
        "    def segment_np(self, cd_long, tau=2):\n",
        "\n",
        "        cd_norm = np.sqrt((cd_long ** 2).sum(axis=-1, keepdims=True))\n",
        "        G = (cd_long @ cd_long.T) / (cd_norm @ cd_norm.T)\n",
        "\n",
        "        L = G.shape[0]\n",
        "\n",
        "        num_rep = []\n",
        "        num_rep_sync = []\n",
        "\n",
        "        prev_boundary = 0\n",
        "        rate = np.random.uniform(0.8, 1.3)\n",
        "\n",
        "        for t in range(1, L+1):\n",
        "            if t==L:\n",
        "                num_rep.append(t - prev_boundary)\n",
        "                num_rep_sync.append(t - prev_boundary)\n",
        "                prev_boundary = t\n",
        "            if t < L:\n",
        "                q = np.random.uniform(rate-0.1, rate)\n",
        "                tmp = G[prev_boundary, max(prev_boundary-20, 0):min(prev_boundary+20, L)]\n",
        "                if q <= 1:\n",
        "                    epsilon = np.quantile(tmp, q)\n",
        "                    if np.all(G[prev_boundary, t:min(t+tau, L)] < epsilon):\n",
        "                        num_rep.append(t - prev_boundary)\n",
        "                        num_rep_sync.append(t - prev_boundary)\n",
        "                        prev_boundary = t\n",
        "                else:\n",
        "                    epsilon = np.quantile(tmp, 2-q)\n",
        "                    if np.all(G[prev_boundary, t:min(t+tau, L)] < epsilon):\n",
        "                        num_rep.append(t - prev_boundary)\n",
        "                    else:\n",
        "                        num_rep.extend([t-prev_boundary-0.5, 0.5])\n",
        "\n",
        "                    num_rep_sync.append(t - prev_boundary)\n",
        "                    prev_boundary = t\n",
        "\n",
        "        num_rep = np.array(num_rep)\n",
        "        num_rep_sync = np.array(num_rep_sync)\n",
        "\n",
        "        return num_rep, num_rep_sync\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Return M uttrs for one spkr.\"\"\"\n",
        "        dataset = self.train_dataset\n",
        "\n",
        "        list_uttrs = dataset[index]\n",
        "\n",
        "        emb_org = list_uttrs[1]\n",
        "\n",
        "        uttr = np.random.randint(2, len(list_uttrs))\n",
        "        melsp, melcep, cd_real = list_uttrs[uttr]\n",
        "\n",
        "        num_rep, num_rep_sync = self.segment_np(cd_real)\n",
        "\n",
        "        return melsp, melcep, cd_real, num_rep, num_rep_sync, len(melsp), len(num_rep), len(num_rep_sync), emb_org\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of spkrs.\"\"\"\n",
        "        return self.num_tokens"
      ],
      "metadata": {
        "id": "H7l7vLT7DZ7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCollator(object):\n",
        "    def __init__(self, hparams):\n",
        "        self.max_len_pad = hparams.max_len_pad\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        new_batch = []\n",
        "\n",
        "        l_short_max = 0\n",
        "        l_short_sync_max = 0\n",
        "        l_real_max = 0\n",
        "\n",
        "        for token in batch:\n",
        "            sp_real, cep_real, cd_real, rep, rep_sync, l_real, l_short, l_short_sync, emb = token\n",
        "\n",
        "            if l_short > l_short_max:\n",
        "                l_short_max = l_short\n",
        "\n",
        "            if l_short_sync > l_short_sync_max:\n",
        "                l_short_sync_max = l_short_sync\n",
        "\n",
        "            if l_real > l_real_max:\n",
        "                l_real_max = l_real\n",
        "\n",
        "            sp_real_pad = np.pad(sp_real, ((0,self.max_len_pad-l_real),(0,0)), 'constant')\n",
        "            cep_real_pad = np.pad(cep_real, ((0,self.max_len_pad-l_real),(0,0)), 'constant')\n",
        "            cd_real_pad = np.pad(cd_real, ((0,self.max_len_pad-l_real),(0,0)), 'constant')\n",
        "\n",
        "            rep_pad = np.pad(rep, (0,self.max_len_pad-l_short), 'constant')\n",
        "            rep_sync_pad = np.pad(rep_sync, (0,self.max_len_pad-l_short_sync), 'constant')\n",
        "\n",
        "            new_batch.append( (sp_real_pad, cep_real_pad, cd_real_pad, rep_pad, rep_sync_pad, l_real, l_short, l_short_sync, emb) )\n",
        "\n",
        "        batch = new_batch\n",
        "\n",
        "        a, b, c, d, e, f, g, h, i = zip(*batch)\n",
        "\n",
        "        sp_real = torch.from_numpy(np.stack(a, axis=0))[:,:l_real_max+1,:]\n",
        "        cep_real = torch.from_numpy(np.stack(b, axis=0))[:,:l_real_max+1,:]\n",
        "        cd_real = torch.from_numpy(np.stack(c, axis=0))[:,:l_real_max+1,:]\n",
        "        num_rep = torch.from_numpy(np.stack(d, axis=0))[:,:l_short_max+1]\n",
        "        num_rep_sync = torch.from_numpy(np.stack(e, axis=0))[:,:l_short_sync_max+1]\n",
        "\n",
        "        len_real = torch.from_numpy(np.stack(f, axis=0))\n",
        "        len_short = torch.from_numpy(np.stack(g, axis=0))\n",
        "        len_short_sync = torch.from_numpy(np.stack(h, axis=0))\n",
        "\n",
        "        spk_emb = torch.from_numpy(np.stack(i, axis=0))\n",
        "\n",
        "        return sp_real, cep_real, cd_real, num_rep, num_rep_sync, len_real, len_short, len_short_sync, spk_emb"
      ],
      "metadata": {
        "id": "VHwQmhWzDh5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiSampler(Sampler):\n",
        "    \"\"\"Samples elements more than once in a single pass through the data.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples, n_repeats, shuffle=False):\n",
        "        self.num_samples = num_samples\n",
        "        self.n_repeats = n_repeats\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def gen_sample_array(self):\n",
        "        self.sample_idx_array = torch.arange(self.num_samples, dtype=torch.int64).repeat(self.n_repeats)\n",
        "        if self.shuffle:\n",
        "            self.sample_idx_array = self.sample_idx_array[torch.randperm(len(self.sample_idx_array))]\n",
        "        return self.sample_idx_array\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.gen_sample_array())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_idx_array)"
      ],
      "metadata": {
        "id": "ai6TyNR-Doop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def worker_init_fn(x):\n",
        "    return np.random.seed((torch.initial_seed()) % (2**32))"
      ],
      "metadata": {
        "id": "-bLJIM9PDrC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(hparams):\n",
        "    \"\"\"Build and return a data loader.\"\"\"\n",
        "\n",
        "    dataset = Utterances(hparams)\n",
        "\n",
        "    my_collator = MyCollator(hparams)\n",
        "\n",
        "    sampler = MultiSampler(len(dataset), hparams.samplier, shuffle=hparams.shuffle)\n",
        "\n",
        "    data_loader = data.DataLoader(dataset=dataset,\n",
        "                                  batch_size=hparams.batch_size,\n",
        "                                  sampler=sampler,\n",
        "                                  num_workers=hparams.num_workers,\n",
        "                                  drop_last=True,\n",
        "                                  pin_memory=False,\n",
        "                                  worker_init_fn=worker_init_fn,\n",
        "                                  collate_fn=my_collator)\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "R1u08q0NDtFQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}