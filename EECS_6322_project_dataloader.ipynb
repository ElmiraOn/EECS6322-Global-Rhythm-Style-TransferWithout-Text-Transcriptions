{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElmiraOn/EECS6322-Global-Rhythm-Style-TransferWithout-Text-Transcriptions/blob/main/EECS_6322_project_dataloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y12tsLgRDXyd"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDRdO-urEgOl"
      },
      "outputs": [],
      "source": [
        "!pip install librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "SY8gdL_3boN0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.fftpack\n",
        "import soundfile as sf\n",
        "from utils import pySTFT\n",
        "from scipy import signal\n",
        "from librosa.filters import mel\n",
        "from utils import butter_highpass\n",
        "from numpy.random import uniform\n",
        "from torch.utils import data\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from multiprocessing import Process, Manager\n",
        "import zipfile\n",
        "\n",
        "from hparams_autopst import hparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PAZHLHsBEKzo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from model_sea import Generator as Model\n",
        "from hparams_sea import hparams as training_hparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H7l7vLT7DZ7y"
      },
      "outputs": [],
      "source": [
        "class Utterances(data.Dataset):\n",
        "    \"\"\"Dataset class for the Utterances dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, hparams):\n",
        "        \"\"\"Initialize and preprocess the Utterances dataset.\"\"\"\n",
        "        self.meta_file = hparams.meta_file\n",
        "\n",
        "        self.feat_dir_1 = hparams.feat_dir_1\n",
        "        self.feat_dir_2 = hparams.feat_dir_2\n",
        "        self.feat_dir_3 = hparams.feat_dir_3\n",
        "\n",
        "        self.step = 4\n",
        "        self.split = 0\n",
        "\n",
        "        self.max_len_pad = hparams.max_len_pad\n",
        "\n",
        "        meta = pickle.load(open(self.meta_file, \"rb\"))\n",
        "\n",
        "        manager = Manager()\n",
        "        meta = manager.list(meta)\n",
        "        dataset = manager.list(len(meta)*[None])  # <-- can be shared between processes.\n",
        "        processes = []\n",
        "        for i in range(0, len(meta), self.step):\n",
        "            p = Process(target=self.load_data,\n",
        "                        args=(meta[i:i+self.step],dataset,i))\n",
        "            p.start()\n",
        "            processes.append(p)\n",
        "        for p in processes:\n",
        "            p.join()\n",
        "\n",
        "        # very importtant to do dataset = list(dataset)\n",
        "        self.train_dataset = list(dataset)\n",
        "        self.num_tokens = len(self.train_dataset)\n",
        "\n",
        "        print('Finished loading the {} Utterances training dataset...'.format(self.num_tokens))\n",
        "\n",
        "\n",
        "    def load_data(self, submeta, dataset, idx_offset):\n",
        "        for k, sbmt in enumerate(submeta):\n",
        "            uttrs = len(sbmt)*[None]\n",
        "            for j, tmp in enumerate(sbmt):\n",
        "                if j < 2:\n",
        "                    # fill in speaker name and embedding\n",
        "                    uttrs[j] = tmp\n",
        "                else:\n",
        "                    # fill in data\n",
        "                    sp_tmp = np.load(os.path.join(self.feat_dir_1, tmp))\n",
        "                    cep_tmp = np.load(os.path.join(self.feat_dir_2, tmp))[:, 0:14]\n",
        "                    cd_tmp = np.load(os.path.join(self.feat_dir_3, tmp))\n",
        "\n",
        "                    assert len(sp_tmp) == len(cep_tmp) == len(cd_tmp)\n",
        "\n",
        "                    uttrs[j] = ( np.clip(sp_tmp, 0, 1), cep_tmp, cd_tmp )\n",
        "            dataset[idx_offset+k] = uttrs\n",
        "\n",
        "\n",
        "    def segment_np(self, cd_long, tau=2):\n",
        "\n",
        "        cd_norm = np.sqrt((cd_long ** 2).sum(axis=-1, keepdims=True))\n",
        "        G = (cd_long @ cd_long.T) / (cd_norm @ cd_norm.T)\n",
        "\n",
        "        L = G.shape[0]\n",
        "\n",
        "        num_rep = []\n",
        "        num_rep_sync = []\n",
        "\n",
        "        prev_boundary = 0\n",
        "        rate = np.random.uniform(0.8, 1.3)\n",
        "\n",
        "        for t in range(1, L+1):\n",
        "            if t==L:\n",
        "                num_rep.append(t - prev_boundary)\n",
        "                num_rep_sync.append(t - prev_boundary)\n",
        "                prev_boundary = t\n",
        "            if t < L:\n",
        "                q = np.random.uniform(rate-0.1, rate)\n",
        "                tmp = G[prev_boundary, max(prev_boundary-20, 0):min(prev_boundary+20, L)]\n",
        "                if q <= 1:\n",
        "                    epsilon = np.quantile(tmp, q)\n",
        "                    if np.all(G[prev_boundary, t:min(t+tau, L)] < epsilon):\n",
        "                        num_rep.append(t - prev_boundary)\n",
        "                        num_rep_sync.append(t - prev_boundary)\n",
        "                        prev_boundary = t\n",
        "                else:\n",
        "                    epsilon = np.quantile(tmp, 2-q)\n",
        "                    if np.all(G[prev_boundary, t:min(t+tau, L)] < epsilon):\n",
        "                        num_rep.append(t - prev_boundary)\n",
        "                    else:\n",
        "                        num_rep.extend([t-prev_boundary-0.5, 0.5])\n",
        "\n",
        "                    num_rep_sync.append(t - prev_boundary)\n",
        "                    prev_boundary = t\n",
        "\n",
        "        num_rep = np.array(num_rep)\n",
        "        num_rep_sync = np.array(num_rep_sync)\n",
        "\n",
        "        return num_rep, num_rep_sync\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Return M uttrs for one spkr.\"\"\"\n",
        "        dataset = self.train_dataset\n",
        "\n",
        "        list_uttrs = dataset[index]\n",
        "\n",
        "        emb_org = list_uttrs[1]\n",
        "\n",
        "        uttr = np.random.randint(2, len(list_uttrs))\n",
        "        melsp, melcep, cd_real = list_uttrs[uttr]\n",
        "\n",
        "        num_rep, num_rep_sync = self.segment_np(cd_real)\n",
        "\n",
        "        return melsp, melcep, cd_real, num_rep, num_rep_sync, len(melsp), len(num_rep), len(num_rep_sync), emb_org\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of spkrs.\"\"\"\n",
        "        return self.num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VHwQmhWzDh5A"
      },
      "outputs": [],
      "source": [
        "class MyCollator(object):\n",
        "    def __init__(self, hparams):\n",
        "        self.max_len_pad = hparams.max_len_pad\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        new_batch = []\n",
        "\n",
        "        l_short_max = 0\n",
        "        l_short_sync_max = 0\n",
        "        l_real_max = 0\n",
        "\n",
        "        for token in batch:\n",
        "            sp_real, cep_real, cd_real, rep, rep_sync, l_real, l_short, l_short_sync, emb = token\n",
        "\n",
        "            if l_short > l_short_max:\n",
        "                l_short_max = l_short\n",
        "\n",
        "            if l_short_sync > l_short_sync_max:\n",
        "                l_short_sync_max = l_short_sync\n",
        "\n",
        "            if l_real > l_real_max:\n",
        "                l_real_max = l_real\n",
        "\n",
        "            sp_real_pad = np.pad(sp_real, ((0,self.max_len_pad-l_real),(0,0)), 'constant')\n",
        "            cep_real_pad = np.pad(cep_real, ((0,self.max_len_pad-l_real),(0,0)), 'constant')\n",
        "            cd_real_pad = np.pad(cd_real, ((0,self.max_len_pad-l_real),(0,0)), 'constant')\n",
        "\n",
        "            rep_pad = np.pad(rep, (0,self.max_len_pad-l_short), 'constant')\n",
        "            rep_sync_pad = np.pad(rep_sync, (0,self.max_len_pad-l_short_sync), 'constant')\n",
        "\n",
        "            new_batch.append( (sp_real_pad, cep_real_pad, cd_real_pad, rep_pad, rep_sync_pad, l_real, l_short, l_short_sync, emb) )\n",
        "\n",
        "        batch = new_batch\n",
        "\n",
        "        a, b, c, d, e, f, g, h, i = zip(*batch)\n",
        "\n",
        "        sp_real = torch.from_numpy(np.stack(a, axis=0))[:,:l_real_max+1,:]\n",
        "        cep_real = torch.from_numpy(np.stack(b, axis=0))[:,:l_real_max+1,:]\n",
        "        cd_real = torch.from_numpy(np.stack(c, axis=0))[:,:l_real_max+1,:]\n",
        "        num_rep = torch.from_numpy(np.stack(d, axis=0))[:,:l_short_max+1]\n",
        "        num_rep_sync = torch.from_numpy(np.stack(e, axis=0))[:,:l_short_sync_max+1]\n",
        "\n",
        "        len_real = torch.from_numpy(np.stack(f, axis=0))\n",
        "        len_short = torch.from_numpy(np.stack(g, axis=0))\n",
        "        len_short_sync = torch.from_numpy(np.stack(h, axis=0))\n",
        "\n",
        "        spk_emb = torch.from_numpy(np.stack(i, axis=0))\n",
        "\n",
        "        return sp_real, cep_real, cd_real, num_rep, num_rep_sync, len_real, len_short, len_short_sync, spk_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ai6TyNR-Doop"
      },
      "outputs": [],
      "source": [
        "class MultiSampler(Sampler):\n",
        "    \"\"\"Samples elements more than once in a single pass through the data.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples, n_repeats, shuffle=False):\n",
        "        self.num_samples = num_samples\n",
        "        self.n_repeats = n_repeats\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def gen_sample_array(self):\n",
        "        self.sample_idx_array = torch.arange(self.num_samples, dtype=torch.int64).repeat(self.n_repeats)\n",
        "        if self.shuffle:\n",
        "            self.sample_idx_array = self.sample_idx_array[torch.randperm(len(self.sample_idx_array))]\n",
        "        return self.sample_idx_array\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.gen_sample_array())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_idx_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-bLJIM9PDrC2"
      },
      "outputs": [],
      "source": [
        "def worker_init_fn(x):\n",
        "    return np.random.seed((torch.initial_seed()) % (2**32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R1u08q0NDtFQ"
      },
      "outputs": [],
      "source": [
        "def get_loader(hparams):\n",
        "    \"\"\"Build and return a data loader.\"\"\"\n",
        "\n",
        "    dataset = Utterances(hparams)\n",
        "\n",
        "    my_collator = MyCollator(hparams)\n",
        "\n",
        "    sampler = MultiSampler(len(dataset), hparams.samplier, shuffle=hparams.shuffle)\n",
        "\n",
        "    data_loader = data.DataLoader(dataset=dataset,\n",
        "                                  batch_size=hparams.batch_size,\n",
        "                                  sampler=sampler,\n",
        "                                  num_workers=hparams.num_workers,\n",
        "                                  drop_last=True,\n",
        "                                  pin_memory=False,\n",
        "                                  worker_init_fn=worker_init_fn,\n",
        "                                  collate_fn=my_collator)\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9L896-8DJKR"
      },
      "source": [
        "Prepare training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U63mDc9FFln",
        "outputId": "f55afa93-d193-4a56-f38e-0310999164e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘assets’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir assets\n",
        "!mkdir assets/vctk16-train-wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "cOv1RqBZDMUt"
      },
      "outputs": [],
      "source": [
        "mel_basis = mel(sr=16000, n_fft=1024, fmin=90, fmax=7600, n_mels=80).T\n",
        "min_level = np.exp(-100 / 20 * np.log(10))\n",
        "b, a = butter_highpass(30, 16000, order=5)\n",
        "\n",
        "mfcc_mean, mfcc_std, dctmx = pickle.load(open('assets/mfcc_stats.pkl', 'rb'))\n",
        "spk2emb = pickle.load(open('assets/spk2emb_82.pkl', 'rb'))\n",
        "\n",
        "rootDir = \"assets/vctk16-train-wav\"\n",
        "targetDir_sp = 'assets/vctk16-train-sp-mel'\n",
        "targetDir_cep = 'assets/vctk16-train-cep-mel'\n",
        "targetDir_cd = 'assets/vctk16-train-teacher'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "G = Model(training_hparams).eval().to(device)\n",
        "\n",
        "#g_checkpoint = torch.load('assets/sea.ckpt', map_location=lambda storage, loc: storage)\n",
        "#G.load_state_dict(g_checkpoint['model'], strict=True)\n",
        "\n",
        "\n",
        "metadata = []\n",
        "dirName, subdirList, fileList = next(os.walk(rootDir))\n",
        "\n",
        "for subdir in sorted(subdirList):\n",
        "    print(subdir)\n",
        "\n",
        "    if not os.path.exists(os.path.join(targetDir_sp, subdir)):\n",
        "        os.makedirs(os.path.join(targetDir_sp, subdir))\n",
        "    if not os.path.exists(os.path.join(targetDir_cep, subdir)):\n",
        "        os.makedirs(os.path.join(targetDir_cep, subdir))\n",
        "    if not os.path.exists(os.path.join(targetDir_cd, subdir)):\n",
        "        os.makedirs(os.path.join(targetDir_cd, subdir))\n",
        "\n",
        "    submeta = []\n",
        "    submeta.append(subdir)\n",
        "    submeta.append(spk2emb[subdir])\n",
        "\n",
        "    _,_, fileList = next(os.walk(os.path.join(dirName,subdir)))\n",
        "\n",
        "    for fileName in sorted(fileList):\n",
        "        x, fs = sf.read(os.path.join(dirName,subdir,fileName))\n",
        "        if x.shape[0] % 256 == 0:\n",
        "            x = np.concatenate((x, np.array([1e-06])), axis=0)\n",
        "        y = signal.filtfilt(b, a, x)\n",
        "        D = pySTFT(y * 0.96).T\n",
        "        D_mel = np.dot(D, mel_basis)\n",
        "        D_db = 20 * np.log10(np.maximum(min_level, D_mel))\n",
        "\n",
        "        # mel sp\n",
        "        S = (D_db + 80) / 100\n",
        "\n",
        "        # mel cep\n",
        "        cc_tmp = S.dot(dctmx)\n",
        "        cc_norm = (cc_tmp - mfcc_mean) / mfcc_std\n",
        "        S = np.clip(S, 0, 1)\n",
        "\n",
        "        # teacher code\n",
        "        cc_torch = torch.from_numpy(cc_norm[:,0:20].astype(np.float32)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            codes = G.encode(cc_torch, torch.ones_like(cc_torch[:,:,0])).squeeze(0)\n",
        "\n",
        "        np.save(os.path.join(targetDir_cd, subdir, fileName[:-4]),\n",
        "                codes.cpu().numpy(), allow_pickle=False)\n",
        "        np.save(os.path.join(targetDir_sp, subdir, fileName[:-4]),\n",
        "                S.astype(np.float32), allow_pickle=False)\n",
        "        np.save(os.path.join(targetDir_cep, subdir, fileName[:-4]),\n",
        "                cc_norm.astype(np.float32), allow_pickle=False)\n",
        "\n",
        "        submeta.append(subdir+'/'+fileName[:-4]+'.npy')\n",
        "\n",
        "    metadata.append(submeta)\n",
        "\n",
        "with open('./assets/train_vctk.meta', 'wb') as handle:\n",
        "    pickle.dump(metadata, handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xly9_0YAhc5w"
      },
      "source": [
        "Install dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqGFxnjUB1wV",
        "outputId": "8484378b-46a3-4147-aec4-e7541b2eb432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "wget is already the newest version (1.21.2-2ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "mkdir: cannot create directory ‘dataset’: File exists\n",
            "--2024-03-29 22:27:17--  https://datashare.ed.ac.uk/bitstream/handle/10283/3443/VCTK-Corpus-0.92.zip\n",
            "Resolving datashare.ed.ac.uk (datashare.ed.ac.uk)... 129.215.67.172\n",
            "Connecting to datashare.ed.ac.uk (datashare.ed.ac.uk)|129.215.67.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 200\n",
            "Length: 11747302977 (11G) [application/zip]\n",
            "Saving to: ‘dataset/VCTK-Corpus-0.92.zip’\n",
            "\n",
            "VCTK-Corpus-0.92.zi 100%[===================>]  10.94G  3.20MB/s    in 51m 8s  \n",
            "\n",
            "2024-03-29 23:18:26 (3.65 MB/s) - ‘dataset/VCTK-Corpus-0.92.zip’ saved [11747302977/11747302977]\n",
            "\n",
            "gzip: stdin has more than one entry--rest ignored\n",
            "tar: This does not look like a tar archive\n",
            "tar: Skipping to next header\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ],
      "source": [
        "!apt-get install wget\n",
        "\n",
        "# Create a directory to store the dataset\n",
        "!mkdir dataset\n",
        "\n",
        "# Use wget to download the dataset to the created directory\n",
        "!wget -P dataset https://datashare.ed.ac.uk/bitstream/handle/10283/3443/VCTK-Corpus-0.92.zip\n",
        "\n",
        "# Extract the downloaded file\n",
        "!tar -xzf dataset/VCTK-Corpus-0.92.zip -C dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2iSOeYBEheWQ"
      },
      "outputs": [],
      "source": [
        "extracted_dir_path = \"dataset/VCTK-Corpus-0.92\"\n",
        "zipfile_dir_path = \"dataset/VCTK-Corpus-0.92.zip\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(extracted_dir_path, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zipfile_dir_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y7sDWfOAXl4",
        "outputId": "010b808c-2162-4fca-8d36-0e3d48775de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished loading the 0 Utterances training dataset...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7e0a31ebc6a0>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataloader = get_loader(hparams)\n",
        "dataloader"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMdDD78mh/YdrjJmaFbIN1w",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
